global:
  security:
    allowInsecureImages: true
  domain: "grafana.ssegning.com"
  storageClassName: longhorn-static


monitoring:
  nameOverride: "monitoring"

  commonAnnotations:
    "kubernetes.io/description": 'Monitoring for Home Cluster'

  keycloak:
    enabled: true
    client_id: "grafana-home"
    client_secret: "<...>"
    url: "https://accounts.ssegning.com"
    realm: "camer-digital"

  prom-stack:
    grafana:
      grafana.ini:
        auth.generic_oauth:
          name: "SSegning"
          role_attribute_path: "contains(resource_access.grafana-home.roles[*], 'admin') && 'Admin' || contains(resource_access.grafana-home.roles[*], 'grafanaadmin') && 'GrafanaAdmin' || 'Viewer'"
          use_refresh_token: "false"
      ingress:
        ingressClassName: traefik
        annotations:
          cert-manager.io/cluster-issuer: cert-cloudflare
  storageClasses:
    '{{ include "common.storage-className" $ }}':
      enabled: false
  loki:
    enabled: false
  alloy:
    ingress:
      enabled: false
      ingressClassName: traefik
      annotations:
        cert-manager.io/cluster-issuer: cert-cloudflare
    alloy:
      configMap:
        content: |-
          // ======================= Grafana Alloy ==============================
          logging {
            level  = "info"
            format = "logfmt"
          }
      
          discovery.kubernetes "pods" {
            role = "pod"
          }
      
          discovery.kubernetes "nodes" {
            role = "node"
          }
      
          discovery.kubernetes "services" {
            role = "service"
          }
      
          discovery.kubernetes "endpoints" {
            role = "endpoints"
          }
      
          discovery.kubernetes "endpointslices" {
            role = "endpointslice"
          }
      
          discovery.kubernetes "ingresses" {
            role = "ingress"
          }
      
          loki.write "default" {
            endpoint {
              url = "http://loki-gateway:80/loki/api/v1/push"
            }
          }
      
          // ─── Node syslog ───────────────────────────────────────────────────
          // local.file_match discovers files on the local filesystem using glob patterns and the doublestar library. It returns an array of file paths.
          local.file_match "node_logs" {
            path_targets = [{
              // Monitor syslog to scrape node-logs
              __path__  = "/var/log/syslog",
              job       = "node/syslog",
              node_name = sys.env("HOSTNAME"),
              cluster   = "main",
            }]
          }
      
          // loki.source.file reads log entries from files and forwards them to other loki.* components.
          // You can specify multiple loki.source.file components by giving them different labels.
          loki.source.file "node_logs" {
            targets    = local.file_match.node_logs.targets
            forward_to = [loki.write.default.receiver]
          }
      
          // ─── Pod/container logs ────────────────────────────────────────────
          // discovery.relabel rewrites the label set of the input targets by applying one or more relabeling rules.
          // If no rules are defined, then the input targets are exported as-is.
          discovery.relabel "pod_logs" {
            targets = discovery.kubernetes.pods.targets
            
            // Label creation - "namespace" field from "__meta_kubernetes_namespace"
            rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action = "replace"
            target_label = "namespace"
          }
            
          // Label creation - "pod" field from "__meta_kubernetes_pod_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            action = "replace"
            target_label = "pod"
          }
            
          // Label creation - "container" field from "__meta_kubernetes_pod_container_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "container"
          }
            
          // Label creation -  "app" field from "__meta_kubernetes_pod_label_app_kubernetes_io_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
            action = "replace"
            target_label = "app"
          }
            
          // Label creation -  "job" field from "__meta_kubernetes_namespace" and "__meta_kubernetes_pod_container_name"
          // Concatenate values __meta_kubernetes_namespace/__meta_kubernetes_pod_container_name
          rule {
            source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
            action        = "replace"
            separator     = "/"
            target_label  = "job"
            replacement   = "$1/$2"
          }
            
          // Label creation - "container" field from "__meta_kubernetes_pod_uid" and "__meta_kubernetes_pod_container_name"
          // Concatenate values __meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name.log
          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            action        = "replace"
            separator     = "/"
            target_label  = "__path__"
            replacement   = "/var/log/pods/*$1/*.log"
          }
            
          // Label creation -  "container_runtime" field from "__meta_kubernetes_pod_container_id"
          rule {
            source_labels = ["__meta_kubernetes_pod_container_id"]
            action        = "replace"
            target_label  = "container_runtime"
            regex         = "^(\\S+):\\/\\/.+$"
            replacement   = "$1"
          }
          }
      
            // loki.source.kubernetes tails logs from Kubernetes containers using the Kubernetes API.
            loki.source.kubernetes "pod_logs" {
            targets    = discovery.relabel.pod_logs.output
            forward_to = [loki.process.pod_logs.receiver]
          }
      
            // loki.process receives log entries from other Loki components, applies one or more processing stages,
            // and forwards the results to the list of receivers in the component's arguments.
            loki.process "pod_logs" {
            stage.static_labels {
            values = {
            cluster = "main",
            }
          }
            
            forward_to = [loki.write.default.receiver]
          }
      
            // ─── Cluster events ────────────────────────────────────────────────
            // loki.source.kubernetes_events tails events from the Kubernetes API and converts them
            // into log lines to forward to other Loki components.
            loki.source.kubernetes_events "cluster_events" {
            job_name   = "integrations/kubernetes/eventhandler"
            log_format = "logfmt"
            forward_to = [
            loki.process.cluster_events.receiver,
          ]
          }
      
            // loki.process receives log entries from other loki components, applies one or more processing stages,
            // and forwards the results to the list of receivers in the component's arguments.
            loki.process "cluster_events" {
            forward_to = [loki.write.default.receiver]
            
            stage.static_labels {
            values = {
            cluster = "main",
            }
          }
            
            stage.labels {
            values = {
            kubernetes_cluster_events = "job",
            }
          }
          }
      
            // ─── OpenTelemetry (OTLP) Gateway ──────────────────────────────────
            otelcol.receiver.otlp "default" {
            http {}
            
            grpc {}
            
            output {
            logs    = [otelcol.processor.batch.logs.input]
            metrics = [otelcol.processor.batch.metrics.input]
            traces  = [otelcol.processor.batch.traces.input]
          }
          }
      
            otelcol.processor.batch "logs"    { output { logs    = [otelcol.exporter.loki.default.input] } }
            otelcol.processor.batch "metrics" { output { metrics = [otelcol.exporter.prometheus.default.input] } }
            otelcol.processor.batch "traces"  { output { traces  = [otelcol.exporter.otlphttp.tempo.input] } }
      
            otelcol.exporter.loki "default" {
            forward_to = [loki.write.default.receiver]
          }
      
            otelcol.exporter.prometheus "default" {
            forward_to = [prometheus.remote_write.mimir.receiver]
          }
      
          otelcol.exporter.otlphttp "tempo" {
            client { endpoint = "tempo-distributor:4318" }
          }
      
          discovery.relabel "scrape_targets" {
            targets = concat(
            discovery.kubernetes.pods.targets,
            )
            
            rule {
            source_labels = [
            "__meta_kubernetes_pod_annotation_prometheus_io_scrape",
            ]
            regex   = "true"
            action  = "keep"
          }
            
          # __metrics_path__ from annotation
          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_path"]
            target_label  = "__metrics_path__"
            regex         = "(.+)"
            action        = "replace"
          }
            
          # :port from annotation
          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_port", "__address__"]
            regex         = "(\\d+);(.+)"
            target_label  = "__address__"
            replacement   = "$2:$1"
            separator     = ";"
            action        = "replace"
          }
            
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action = "replace"
            target_label = "namespace"
          }
          }
      
          prometheus.scrape "k8s" {
            targets = discovery.relabel.scrape_targets.output
            forward_to = [ prometheus.remote_write.mimir.receiver ]
          }
      
          prometheus.remote_write "mimir" {
            endpoint {
            url = "http://mimir-nginx:80/api/v1/push"
          }
          }
          

      extraPorts:
        - name: "otel-http"
          port: 4318
          targetPort: 4318
          protocol: "TCP"
        - name: "otel"
          port: 4317
          targetPort: 4317
          protocol: "TCP"
    extraObjects:
      - apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          annotations:
            cert-manager.io/cluster-issuer: cert-cloudflare
          name: monitoring-alloy-otel
          namespace: monitoring
        spec:
          ingressClassName: traefik
          rules:
            - host: otel-alloy-grafana.ssegning.com
              http:
                paths:
                  - backend:
                      service:
                        name: monitoring-alloy
                        port:
                          number: 4318
                    path: /
                    pathType: Prefix



loki:
  fullnameOverride: loki
  enabled: false
  memberlist:
    service:
      publishNotReadyAddresses: true
  ingress:
    enabled: true
    ingressClassName: traefik
    hosts:
      - "loki-{{ .Values.global.domain }}"
  loki:
    auth_enabled: false
    storage:
      type: filesystem
      bucketNames:
        chunks: monitoring
        ruler: monitoring
      object_store:
        storage_prefix: "loki_"
    structuredConfig:
      #compactor:
      #  retention_enabled: true
      #  delete_request_store: s3
      limits_config:
        retention_period: 30d

    commonConfig:
      replication_factor: 1
    #schemaConfig:
    #  configs:
    #    - from: "2024-04-01"
    #      store: tsdb
    #      object_store: s3
    #      schema: v13
    #      index:
    #        prefix: loki_index_
    #        period: 24h
    pattern_ingester:
      enabled: true
    limits_config:
      allow_structured_metadata: true
      volume_enabled: true
  ruler:
    enable_api: true
  lokiCanary:
    enabled: false
  test:
    enabled: false
  monitoring:
    dashboards:
      enabled: false
    rules:
      enabled: false
    serviceMonitor:
      enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    lokiCanary:
      enabled: false

  minio:
    enabled: false

  deploymentMode: SingleBinary

  singleBinary:
    replicas: 1
    resources:
      requests:
        cpu: '100m'
        memory: '600Mi'
      limits:
        cpu: '1000m'
        memory: '4Gi'
    persistence:
      storageClass: 'longhorn-static'

  # Zero out replica counts of other deployment modes
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0

  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0