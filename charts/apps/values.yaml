argoCdConfigs:
  secretMap:
    vaam-eat-aio-repo:
      type: repository
      data:
        url: "https://github.com/vaam-store/vaam-eat-aio"
        name: vaam-eat-aio
        type: git
    apple-health-export:
      type: repository
      data:
        url: "https://whythatfunction.github.io/apple-health-export"
        name: apple-health-export
        type: helm

applications:
#  - name: vaam-eat-aio
#    finalizers:
#      - resources-finalizer.argocd.argoproj.io
#    project: external
#    source:
#      repoURL: https://github.com/vaam-store/vaam-eat-aio
#      targetRevision: HEAD
#      path: charts/serverless
#      helm:
#        valuesObject:
#          global:
#            imagePullSecrets:
#              - ghcr-login-secret
#          image:
#            tag: "main-181639c"
#          labels:
#            environment: prod
#          db:
#            storage:
#              size: 5Gi
#              storageClass: longhorn
#          env:
#            AUTH_SECRET:
#              secretKeyRef:
#                name: 'vaam-eat-config'
#                key: secret
#
#            S3_ENDPOINT: s3-minio.s3.svc.cluster.local
#            S3_ACCESS_KEY:
#              secretKeyRef:
#                name: 'vaam-eat-config'
#                key: minio-user
#            S3_SECRET_KEY:
#              secretKeyRef:
#                name: 'vaam-eat-config'
#                key: minio-password
#            S3_PORT: 9000
#            S3_SCHEME: http
#            S3_BUCKET: vaam-eat
#            S3_CDN_URL:
#              configMapKeyRef:
#                name: 'vaam-eat-config'
#                key: s3-cdn
#
#            NEXT_PUBLIC_MAPS_PMTILES_MINIO_BASE_URL:
#              configMapKeyRef:
#                name: 'vaam-eat-config'
#                key: s3-cdn
#            
#            NEXT_PUBLIC_MAPS_PMTILES_MINIO_BUCKET: 'vaam-eat'
#
#            OTEL_EXPORTER_OTLP_ENDPOINT: 'http://tempo.monitoring.svc.cluster.local:4318'
#            OTEL_EXPORTER_OTLP_PROTOCOL: 'http'
#
#            REDIS_URL: 'redis://redis-master.database.svc.cluster.local:6379'
#            REDIS_PREFIX: 'eat_vaam_store_'
#            
#            SKIP_ENV_VALIDATION: 1
#
#            EMAIL_SERVER_HOST: 'mail.mail.svc.cluster.local'
#            EMAIL_SERVER_PORT: 587
#            EMAIL_SERVER_USER: ""
#            EMAIL_SERVER_PASSWORD: ""
#            EMAIL_FROM: 'no-reply@mail.vaam.store'
#
#            APP_URL:
#              configMapKeyRef:
#                name: 'vaam-eat-config'
#                key: public-url
#            NEXTAUTH_URL:
#              configMapKeyRef:
#                name: 'vaam-eat-config'
#                key: public-url
#            
#            EMGR_CDN: "https://emgr.vaam.store/api/images/resize"
#          
#          migration:
#            image:
#              tag: "main-181639c"
#          domain:
#            enabled: true
#            name: eat.vaam.store
#            tls:
#              issuerRef:
#                kind: ClusterIssuer
#                name: cert-cloudflare
#                group: cert-manager.io
#    destination:
#      server: https://kubernetes.default.svc
#      namespace: ext-vaam-eat
#    syncPolicy:
#      automated:
#        prune: true
#        selfHeal: true

#  - name: apple-health-export-serverless
#    finalizers:
#      - resources-finalizer.argocd.argoproj.io
#    project: external
#    source:
#      repoURL: https://whythatfunction.github.io/apple-health-export
#      targetRevision: 1.0.0
#      chart: apple-health-export-serverless
#      helm:
#        valuesObject:
#          env:
#            AHE_BUCKET: apple-health-export
#            AHE_PREFIX: "reports/"
#            
#            AHE_S3_PATH_STYLE: "true"
#
#            AHE_BASIC_USER:
#              secretKeyRef:
#                name: apple-health-export-config
#                key: basic-username
#
#            AHE_BASIC_PASS:
#              secretKeyRef:
#                name: apple-health-export-config
#                key: basic-password
#
#            AWS_ENDPOINT_URL: http://s3-minio.s3.svc.cluster.local:9000
#            AWS_ALLOW_HTTP: "true"
#            AWS_REGION: ~
#            AWS_ACCESS_KEY_ID:
#              secretKeyRef:
#                name: apple-health-export-config
#                key: minio-user
#
#            AWS_SECRET_ACCESS_KEY:
#              secretKeyRef:
#                name: apple-health-export-config
#                key: minio-password
#            
#            RUST_LOG: "info,aws_config=warn,aws_smithy_runtime=warn,hyper=warn,tower_http=info"
#            OTEL_EXPORTER_OTLP_ENDPOINT: 'http://tempo.monitoring.svc.cluster.local:4317'
#    destination:
#      server: https://kubernetes.default.svc
#      namespace: ext-apple-health-export
#    syncPolicy:
#      syncOptions:
#        - ServerSideApply=true
#      automated:
#        prune: true
#        selfHeal: true

#  - name: vaam-emgr
#    finalizers:
#      - resources-finalizer.argocd.argoproj.io
#    project: external
#    source:
#      repoURL: https://vaam-store.github.io/image-resizer
#      targetRevision: 0.1.3
#      chart: emgr-serverless
#      helm:
#        valuesObject:
#          image:
#            tag: "s3_otel-latest"
#          env:
#            MINIO_ACCESS_KEY_ID:
#              secretKeyRef:
#                name: 'vaam-eat-config'
#                key: minio-user
#            
#            MINIO_SECRET_ACCESS_KEY:
#              secretKeyRef:
#                name: 'vaam-eat-config'
#                key: minio-password
#            
#            MINIO_ENDPOINT_URL: http://s3-minio.s3.svc.cluster.local:9000
#            MINIO_BUCKET: vaam-eat
#            
#            STORAGE_SUB_PATH: 'images/gen/'
#            
#            CDN_BASE_URL: https://s3.ssegning.me/vaam-eat
#            
#            LOG_LEVEL: info
#            OTLP_SPAN_ENDPOINT: http://tempo.monitoring.svc.cluster.local:4317
#            OTLP_METRIC_ENDPOINT: http://tempo.monitoring.svc.cluster.local:4318/v1/metrics
#            OTLP_SERVICE_NAME: vaam-emgr
#            ENABLE_HTTP2: "false"
#            PERFORMANCE_PROFILE: high_throughput
#            MAX_CONCURRENT_PROCESSING: 8
#          
#          domain:
#            enabled: true
#            name: emgr.vaam.store
#            tls:
#              issuerRef:
#                kind: ClusterIssuer
#                name: cert-cloudflare
#                group: cert-manager.io
#    destination:
#      server: https://kubernetes.default.svc
#      namespace: ext-vaam-eat
#    syncPolicy:
#      automated:
#        prune: true
#        selfHeal: true

  - name: jellyfin
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/jellyfin
    destination:
      server: https://kubernetes.default.svc
      namespace: jellyfin
    syncPolicy:
      syncOptions:
        - CreateNamespace=true
      automated:
        prune: true
        selfHeal: true

  - name: keycloak
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/keycloak
    destination:
      server: https://kubernetes.default.svc
      namespace: keycloak
    syncPolicy:
      automated:
        prune: true
        selfHeal: true

  - name: remote-code
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/remote-code
    destination:
      server: https://kubernetes.default.svc
      namespace: remote-code
    syncPolicy:
      syncOptions:
        - ServerSideApply=true
      automated:
        prune: true
        selfHeal: true

  - name: password-manager
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/password-manager
    destination:
      server: https://kubernetes.default.svc
      namespace: password-manager
    syncPolicy:
      automated:
        prune: true
        selfHeal: true
  - name: endpoints
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/endpoints
    destination:
      server: https://kubernetes.default.svc
      namespace: endpoints
    syncPolicy:
      automated:
        prune: true
        selfHeal: true
  - name: music
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/music
    destination:
      server: https://kubernetes.default.svc
      namespace: music
    syncPolicy:
      syncOptions:
        - CreateNamespace=true
      automated:
        prune: true
        selfHeal: true

  - name: tigerbeetle
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://vymalo.github.io/tigerbeetle-helm-charts
      targetRevision: 0.9.6
      chart: tigerbeetle
      helm:
        valuesObject:
          secrets:
            config:
              enabled: true
              stringData:
                CLUSTER_ID: "222519218452505945768636437399044307163"
          controllers:
            main:
              initContainers:
                resolve-addresses:
                  env:
                    POD_NAMESPACE:
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                  args:
                    - |
                      #!/usr/bin/env bash
                      set -euo pipefail
                      
                      : "${RAW_ADDRESSES:?comma-separated host:port list}"
                      # namespace of the *service*; defaults to our own ns, but override if needed
                      SVC_NAMESPACE="${TARGET_NAMESPACE:-${POD_NAMESPACE:-}}"
                      CLUSTER_DOMAIN="${CLUSTER_DOMAIN:-svc.cluster.local}"
                      RESOLVE_TIMEOUT_SECS="${RESOLVE_TIMEOUT_SECS:-90}"
                      SLEEP_SECS="${SLEEP_SECS:-3}"
                      
                      fqdn_for() {
                        local host="$1"
                        # If host already looks FQDN-ish, return as is
                        [[ "$host" == *".svc."* ]] && { echo "$host"; return; }
                        if [[ -n "$SVC_NAMESPACE" ]]; then
                          echo "${host}.${SVC_NAMESPACE}.${CLUSTER_DOMAIN}"
                        else
                          # fall back to relying on search domains
                          echo "$host"
                        fi
                      }
                      
                      resolve_ips_with_retry() {
                        local name="$1"
                        local deadline=$(( $(date +%s) + RESOLVE_TIMEOUT_SECS ))
                        local out=""
                        while (( $(date +%s) < deadline )); do
                          # use getent hosts (portable) and dedup
                          out="$(getent hosts "$name" | awk '{print $1}' | sort -u || true)"
                          if [[ -n "$out" ]]; then
                            printf '%s\n' "$out"
                            return 0
                          fi
                          sleep "$SLEEP_SECS"
                        done
                        return 1
                      }
                      
                      OUT=""
                      IFS=',' read -r -a ITEMS <<< "$RAW_ADDRESSES"
                      for item in "${ITEMS[@]}"; do
                        host="${item%%:*}"
                        port="${item##*:}"
                        printf 'Looking for %s:%s\n' "$host" "$port"
                      
                        name="$(fqdn_for "$host")"
                        if ! mapfile -t IPS < <(resolve_ips_with_retry "$name"); then
                          printf 'WARN: could not resolve %s (queried: %s; ns=%s)\n' "$host" "$name" "${SVC_NAMESPACE:-<inherit>}"
                          continue
                        fi
                      
                        for ip in "${IPS[@]}"; do
                          printf 'Found %s for %s:%s\n' "$ip" "$host" "$port"
                          OUT="${OUT:+$OUT,}${ip}:${port}"
                        done
                      done
                      
                      printf '%s\n' "$OUT" > /work/addresses.txt
                      printf 'Resolved: %s\n' "$OUT"

              statefulset:
                volumeClaimTemplates:
                  - name: data
                    mountPath: /data
                    accessMode: "ReadWriteOnce"
                    size: 5Gi
                    storageClass: "longhorn-local"
    destination:
      server: https://kubernetes.default.svc
      namespace: ext-vymalo-tigerbeetle
    syncPolicy:
      syncOptions:
        - CreateNamespace=true
      automated:
        prune: true
        selfHeal: true

#  - name: n8n-apps
#    finalizers:
#      - resources-finalizer.argocd.argoproj.io
#    project: external
#    source:
#      repoURL: https://github.com/whythatfunction/home-os
#      targetRevision: HEAD
#      path: charts/home-apps/n8n-apps
#    destination:
#      server: https://kubernetes.default.svc
#      namespace: n8n
#    syncPolicy:
#      syncOptions:
#        - CreateNamespace=true
#        - ServerSideApply=true
#      automated:
#        prune: true
#        selfHeal: true
#        
#  - name: nocodb-apps
#    finalizers:
#      - resources-finalizer.argocd.argoproj.io
#    project: external
#    source:
#      repoURL: https://github.com/whythatfunction/home-os
#      targetRevision: HEAD
#      path: charts/home-apps/nocodb
#    destination:
#      server: https://kubernetes.default.svc
#      namespace: nocodb
#    syncPolicy:
#      syncOptions:
#        - CreateNamespace=true
#        - ServerSideApply=true
#      automated:
#        prune: true
#        selfHeal: true