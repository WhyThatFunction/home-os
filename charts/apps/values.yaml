argoCdConfigs:
  secretMap:
    vaam-eat-aio-repo:
      type: repository
      data:
        url: "https://github.com/vaam-store/vaam-eat-aio"
        name: vaam-eat-aio
        type: git
    apple-health-export:
      type: repository
      data:
        url: "https://whythatfunction.github.io/apple-health-export"
        name: apple-health-export
        type: helm

applications:
  #  - name: vaam-eat-aio
  #    finalizers:
  #      - resources-finalizer.argocd.argoproj.io
  #    project: external
  #    source:
  #      repoURL: https://github.com/vaam-store/vaam-eat-aio
  #      targetRevision: HEAD
  #      path: charts/serverless
  #      helm:
  #        valuesObject:
  #          global:
  #            imagePullSecrets:
  #              - ghcr-login-secret
  #          image:
  #            tag: "main-181639c"
  #          labels:
  #            environment: prod
  #          db:
  #            storage:
  #              size: 5Gi
  #              storageClass: longhorn
  #          env:
  #            AUTH_SECRET:
  #              secretKeyRef:
  #                name: 'vaam-eat-config'
  #                key: secret
  #
  #            S3_ENDPOINT: s3-minio.s3.svc.cluster.local
  #            S3_ACCESS_KEY:
  #              secretKeyRef:
  #                name: 'vaam-eat-config'
  #                key: minio-user
  #            S3_SECRET_KEY:
  #              secretKeyRef:
  #                name: 'vaam-eat-config'
  #                key: minio-password
  #            S3_PORT: 9000
  #            S3_SCHEME: http
  #            S3_BUCKET: vaam-eat
  #            S3_CDN_URL:
  #              configMapKeyRef:
  #                name: 'vaam-eat-config'
  #                key: s3-cdn
  #
  #            NEXT_PUBLIC_MAPS_PMTILES_MINIO_BASE_URL:
  #              configMapKeyRef:
  #                name: 'vaam-eat-config'
  #                key: s3-cdn
  #            
  #            NEXT_PUBLIC_MAPS_PMTILES_MINIO_BUCKET: 'vaam-eat'
  #
  #            OTEL_EXPORTER_OTLP_ENDPOINT: 'http://tempo.monitoring.svc.cluster.local:4318'
  #            OTEL_EXPORTER_OTLP_PROTOCOL: 'http'
  #
  #            REDIS_URL: 'redis://redis-master.database.svc.cluster.local:6379'
  #            REDIS_PREFIX: 'eat_vaam_store_'
  #            
  #            SKIP_ENV_VALIDATION: 1
  #
  #            EMAIL_SERVER_HOST: 'mail.mail.svc.cluster.local'
  #            EMAIL_SERVER_PORT: 587
  #            EMAIL_SERVER_USER: ""
  #            EMAIL_SERVER_PASSWORD: ""
  #            EMAIL_FROM: 'no-reply@mail.vaam.store'
  #
  #            APP_URL:
  #              configMapKeyRef:
  #                name: 'vaam-eat-config'
  #                key: public-url
  #            NEXTAUTH_URL:
  #              configMapKeyRef:
  #                name: 'vaam-eat-config'
  #                key: public-url
  #            
  #            EMGR_CDN: "https://emgr.vaam.store/api/images/resize"
  #          
  #          migration:
  #            image:
  #              tag: "main-181639c"
  #          domain:
  #            enabled: true
  #            name: eat.vaam.store
  #            tls:
  #              issuerRef:
  #                kind: ClusterIssuer
  #                name: cert-cloudflare
  #                group: cert-manager.io
  #    destination:
  #      server: https://kubernetes.default.svc
  #      namespace: ext-vaam-eat
  #    syncPolicy:
  #      automated:
  #        prune: true
  #        selfHeal: true
  
  #  - name: apple-health-export-serverless
  #    finalizers:
  #      - resources-finalizer.argocd.argoproj.io
  #    project: external
  #    source:
  #      repoURL: https://whythatfunction.github.io/apple-health-export
  #      targetRevision: 1.0.0
  #      chart: apple-health-export-serverless
  #      helm:
  #        valuesObject:
  #          env:
  #            AHE_BUCKET: apple-health-export
  #            AHE_PREFIX: "reports/"
  #            
  #            AHE_S3_PATH_STYLE: "true"
  #
  #            AHE_BASIC_USER:
  #              secretKeyRef:
  #                name: apple-health-export-config
  #                key: basic-username
  #
  #            AHE_BASIC_PASS:
  #              secretKeyRef:
  #                name: apple-health-export-config
  #                key: basic-password
  #
  #            AWS_ENDPOINT_URL: http://s3-minio.s3.svc.cluster.local:9000
  #            AWS_ALLOW_HTTP: "true"
  #            AWS_REGION: ~
  #            AWS_ACCESS_KEY_ID:
  #              secretKeyRef:
  #                name: apple-health-export-config
  #                key: minio-user
  #
  #            AWS_SECRET_ACCESS_KEY:
  #              secretKeyRef:
  #                name: apple-health-export-config
  #                key: minio-password
  #            
  #            RUST_LOG: "info,aws_config=warn,aws_smithy_runtime=warn,hyper=warn,tower_http=info"
  #            OTEL_EXPORTER_OTLP_ENDPOINT: 'http://tempo.monitoring.svc.cluster.local:4317'
  #    destination:
  #      server: https://kubernetes.default.svc
  #      namespace: ext-apple-health-export
  #    syncPolicy:
  #      syncOptions:
  #        - ServerSideApply=true
  #      automated:
  #        prune: true
  #        selfHeal: true
  
  #  - name: vaam-emgr
  #    finalizers:
  #      - resources-finalizer.argocd.argoproj.io
  #    project: external
  #    source:
  #      repoURL: https://vaam-store.github.io/image-resizer
  #      targetRevision: 0.1.3
  #      chart: emgr-serverless
  #      helm:
  #        valuesObject:
  #          image:
  #            tag: "s3_otel-latest"
  #          env:
  #            MINIO_ACCESS_KEY_ID:
  #              secretKeyRef:
  #                name: 'vaam-eat-config'
  #                key: minio-user
  #            
  #            MINIO_SECRET_ACCESS_KEY:
  #              secretKeyRef:
  #                name: 'vaam-eat-config'
  #                key: minio-password
  #            
  #            MINIO_ENDPOINT_URL: http://s3-minio.s3.svc.cluster.local:9000
  #            MINIO_BUCKET: vaam-eat
  #            
  #            STORAGE_SUB_PATH: 'images/gen/'
  #            
  #            CDN_BASE_URL: https://s3.ssegning.me/vaam-eat
  #            
  #            LOG_LEVEL: info
  #            OTLP_SPAN_ENDPOINT: http://tempo.monitoring.svc.cluster.local:4317
  #            OTLP_METRIC_ENDPOINT: http://tempo.monitoring.svc.cluster.local:4318/v1/metrics
  #            OTLP_SERVICE_NAME: vaam-emgr
  #            ENABLE_HTTP2: "false"
  #            PERFORMANCE_PROFILE: high_throughput
  #            MAX_CONCURRENT_PROCESSING: 8
  #          
  #          domain:
  #            enabled: true
  #            name: emgr.vaam.store
  #            tls:
  #              issuerRef:
  #                kind: ClusterIssuer
  #                name: cert-cloudflare
  #                group: cert-manager.io
  #    destination:
  #      server: https://kubernetes.default.svc
  #      namespace: ext-vaam-eat
  #    syncPolicy:
  #      automated:
  #        prune: true
  #        selfHeal: true

  - name: jellyfin
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/jellyfin
    destination:
      server: https://kubernetes.default.svc
      namespace: jellyfin
    syncPolicy:
      syncOptions:
        - CreateNamespace=true
      automated:
        prune: true
        selfHeal: true

  - name: keycloak
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/keycloak
    destination:
      server: https://kubernetes.default.svc
      namespace: keycloak
    syncPolicy:
      automated:
        prune: true
        selfHeal: true

  - name: remote-code
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/remote-code
    destination:
      server: https://kubernetes.default.svc
      namespace: remote-code
    syncPolicy:
      syncOptions:
        - ServerSideApply=true
      automated:
        prune: true
        selfHeal: true

  - name: password-manager
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/password-manager
    destination:
      server: https://kubernetes.default.svc
      namespace: password-manager
    syncPolicy:
      automated:
        prune: true
        selfHeal: true
  - name: endpoints
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/endpoints
    destination:
      server: https://kubernetes.default.svc
      namespace: endpoints
    syncPolicy:
      automated:
        prune: true
        selfHeal: true
  - name: music
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://github.com/whythatfunction/home-os
      targetRevision: HEAD
      path: charts/home-apps/music
    destination:
      server: https://kubernetes.default.svc
      namespace: music
    syncPolicy:
      syncOptions:
        - CreateNamespace=true
      automated:
        prune: true
        selfHeal: true

  - name: tigerbeetle
    finalizers:
      - resources-finalizer.argocd.argoproj.io
    project: external
    source:
      repoURL: https://vymalo.github.io/tigerbeetle-helm-charts
      targetRevision: 0.9.8
      chart: tigerbeetle
      helm:
        valuesObject:
          defaultPodOptions:
            automountServiceAccountToken: true
          secrets:
            config:
              enabled: true
              stringData:
                CLUSTER_ID: "222519218452505945768636437399044307163"
          controllers:
            main:
              initContainers:
                resolve-addresses:
                  args: |-
                    set -eu
                    log(){ printf '%s %s\n' "$(date -Iseconds)" "$*"; }
                    
                    : "${NS:?}"; : "${SVC:?}"
                    RESOLVE_TIMEOUT_SECS="${RESOLVE_TIMEOUT_SECS:-60}"
                    SLEEP_SECS="${SLEEP_SECS:-2}"
                    PORT="${PORT:-}"
                    PORT_NAME="${PORT_NAME:-}"
                    IP_FAMILY="${IP_FAMILY:-}"
                    
                    log "kubectl: $(kubectl version --client=true --short 2>/dev/null || echo n/a)"
                    log "Namespace=$NS Service=$SVC PORT=${PORT:-<unset>} PORT_NAME=${PORT_NAME:-<unset>} IP_FAMILY=${IP_FAMILY:-both}"
                    
                    # Show current service + endpoints for context
                    kubectl -n "$NS" get svc "$SVC" -o wide || true
                    kubectl -n "$NS" get endpoints "$SVC" -o wide || true
                    kubectl -n "$NS" get endpointslices -l "kubernetes.io/service-name=$SVC" || true
                    
                    # Optionally resolve port by name
                    if [ -z "$PORT" ] && [ -n "$PORT_NAME" ]; then
                      PORT="$(kubectl -n "$NS" get svc "$SVC" -o jsonpath="{.spec.ports[?(@.name=='$PORT_NAME')].port}" || true)"
                      log "Resolved PORT_NAME='$PORT_NAME' -> PORT='$PORT'"
                    fi
                    [ -n "$PORT" ] || { log "ERROR: no PORT or PORT_NAME resolved"; exit 1; }
                    
                    end=$(( $(date +%s) + RESOLVE_TIMEOUT_SECS ))
                    attempt=0
                    while [ "$(date +%s)" -lt "$end" ]; do
                      attempt=$((attempt+1))
                      # Prefer classic Endpoints (includes notReadyAddresses)
                      EP_IPS="$(kubectl -n "$NS" get endpoints "$SVC" \
                                 -o jsonpath='{range .subsets[*].addresses[*]}{.ip}{"\n"}{end}{range .subsets[*].notReadyAddresses[*]}{.ip}{"\n"}{end}' 2>/dev/null || true)"
                      # Fall back to EndpointSlices
                      if [ -z "$EP_IPS" ]; then
                        EP_IPS="$(kubectl -n "$NS" get endpointslices -l "kubernetes.io/service-name=$SVC" \
                                   -o jsonpath='{range .items[*].endpoints[*].addresses[*]}{.}{"\n"}{end}' 2>/dev/null || true)"
                      fi
                    
                      # Normalize
                      EP_IPS="$(printf '%s\n' "$EP_IPS" | sed '/^$/d' | sort -u)"
                      case "$IP_FAMILY" in
                        ipv4) EP_IPS="$(printf '%s\n' "$EP_IPS" | grep -E '^[0-9.]+$' || true)";;
                        ipv6) EP_IPS="$(printf '%s\n' "$EP_IPS" | grep -E '^[0-9a-fA-F:]+$' || true)";;
                        *) :;;
                      esac
                    
                      COUNT="$(printf '%s\n' "$EP_IPS" | sed '/^$/d' | wc -l | tr -d ' ')"
                      log "Attempt #$attempt: found $COUNT IP(s)"
                      if [ "$COUNT" -gt 0 ]; then
                        OUT="$(printf '%s\n' "$EP_IPS" | awk -v p="$PORT" 'BEGIN{ORS=""; c=0} NF{if(c++){printf ","}; printf $0 ":" p}')"
                        printf '%s\n' "$OUT" > /work/addresses.txt
                        log "Resolved (API): $OUT"
                        exit 0
                      fi
                    
                      sleep "$SLEEP_SECS"
                    done
                    
                    log "ERROR: no endpoints found for Service '$SVC' in namespace '$NS'"
                    exit 1
              containers:
                addresses-watcher:
                  args: |
                    set -eux
                    log(){ printf '%s %s\n' "$(date -Iseconds)" "$*"; }
                    
                    : "${NS:?}"; : "${SVC:?}"; : "${PORT:?}"
                    IP_FAMILY="${IP_FAMILY:-}"
                    POLL_SECS="${POLL_SECS:-10}"
                    RELOAD_URL="${RELOAD_URL:-}"
                    
                    out=/work/addresses.txt
                    tmp=/work/.addresses.new
                    
                    build(){
                      EP_IPS="$(kubectl -n "$NS" get endpoints "$SVC" \
                                 -o jsonpath='{range .subsets[*].addresses[*]}{.ip}{"\n"}{end}{range .subsets[*].notReadyAddresses[*]}{.ip}{"\n"}{end}' 2>/dev/null || true)"
                      if [ -z "$EP_IPS" ]; then
                        EP_IPS="$(kubectl -n "$NS" get endpointslices -l "kubernetes.io/service-name=$SVC" \
                                   -o jsonpath='{range .items[*].endpoints[*].addresses[*]}{.}{"\n"}{end}' 2>/dev/null || true)"
                      fi
                      EP_IPS="$(printf '%s\n' "$EP_IPS" | sed '/^$/d' | sort -u)"
                      case "$IP_FAMILY" in
                        ipv4) EP_IPS="$(printf '%s\n' "$EP_IPS" | grep -E '^[0-9.]+$' || true)";;
                        ipv6) EP_IPS="$(printf '%s\n' "$EP_IPS" | grep -E '^[0-9a-fA-F:]+$' || true)";;
                        *) :;;
                      esac
                      printf '%s\n' "$EP_IPS" | awk -v p="$PORT" 'BEGIN{ORS=""; c=0} NF{if(c++){printf ","}; printf $0 ":" p}' > "$tmp"
                    }
                    
                    ensure_out(){
                      [ -f "$out" ] || { echo > "$out"; }
                    }
                    
                    ensure_out
                    prev="$(cat "$out")"
                    log "Watcher starting; initial content: ${prev:-<empty>}"
                    while :; do
                      build
                      now="$(cat "$tmp")"
                      if ! cmp -s "$tmp" "$out"; then
                        mv -f "$tmp" "$out"
                        log "Addresses changed -> $now"
                        if [ -n "$RELOAD_URL" ]; then
                          # Optional: notify the app if it supports an HTTP reload
                          wget -qO- --timeout=2 "$RELOAD_URL" >/dev/null 2>&1 || log "Reload notify failed (ignored)"
                        fi
                      fi
                      sleep "$POLL_SECS"
                    done

              statefulset:
                volumeClaimTemplates:
                  - name: data
                    mountPath: /data
                    accessMode: "ReadWriteOnce"
                    size: 5Gi
                    storageClass: "longhorn-local"
    destination:
      server: https://kubernetes.default.svc
      namespace: ext-vymalo-tigerbeetle
    syncPolicy:
      syncOptions:
        - CreateNamespace=true
      automated:
        prune: true
        selfHeal: true

#  - name: n8n-apps
#    finalizers:
#      - resources-finalizer.argocd.argoproj.io
#    project: external
#    source:
#      repoURL: https://github.com/whythatfunction/home-os
#      targetRevision: HEAD
#      path: charts/home-apps/n8n-apps
#    destination:
#      server: https://kubernetes.default.svc
#      namespace: n8n
#    syncPolicy:
#      syncOptions:
#        - CreateNamespace=true
#        - ServerSideApply=true
#      automated:
#        prune: true
#        selfHeal: true
#        
#  - name: nocodb-apps
#    finalizers:
#      - resources-finalizer.argocd.argoproj.io
#    project: external
#    source:
#      repoURL: https://github.com/whythatfunction/home-os
#      targetRevision: HEAD
#      path: charts/home-apps/nocodb
#    destination:
#      server: https://kubernetes.default.svc
#      namespace: nocodb
#    syncPolicy:
#      syncOptions:
#        - CreateNamespace=true
#        - ServerSideApply=true
#      automated:
#        prune: true
#        selfHeal: true